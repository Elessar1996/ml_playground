{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from torchvision.ops import nms, RoIPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import display\n",
    "from src import data_transformer as dt\n",
    "from src.evaluation import jaccard, non_max_suppression, PredBoundingBox, MAP\n",
    "from src.augmentations import RandomHorizontalFlip, RandomContrast\n",
    "from src.faster_rcnn_utils import generator_anchors, loc2bbox, bbox2loc, random_choice, normal_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_PATH = \"./data/VOCdevkit/VOC2012/JPEGImages/\"\n",
    "train_json_path = \"./data/VOCdevkit/VOC2012/cocoformatJson/voc_2012_train.json\"\n",
    "val_json_path = \"./data/VOCdevkit/VOC2012/cocoformatJson/voc_2012_val.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = 224\n",
    "num_classes = 20\n",
    "\n",
    "seed = 42\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "id_cat, temp_list = dt.load_pascal(train_json_path)\n",
    "data_list += temp_list\n",
    "_, temp_list = dt.load_pascal(val_json_path)\n",
    "data_list += temp_list\n",
    "\n",
    "random.seed(seed)\n",
    "random.shuffle(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = dt.rescale_bounding_boxes(data_list, target_size)\n",
    "data_list = dt.convert_to_center(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = data_list[0]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_str = IMG_PATH + example.filename\n",
    "img = display.read_img(img_str, target_size)\n",
    "\n",
    "img = display.draw_boxes(img, example.bounding_boxes)\n",
    "img = display.draw_text(img, example.classnames, example.bounding_boxes)\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_to_minmax(bbox):\n",
    "    xmin = bbox[:, 0] - 0.5 * bbox[:, 2]\n",
    "    xmax = bbox[:, 0] + 0.5 * bbox[:, 2]\n",
    "\n",
    "    ymin = bbox[:, 0] - 0.5 * bbox[:, 2]\n",
    "    ymax = bbox[:, 0] + 0.5 * bbox[:, 2]\n",
    "    return torch.stack([xmin, ymin, xmax, ymax], dim=1)\n",
    "\n",
    "\n",
    "class PascalData(Dataset):\n",
    "    def __init__(self, data_list_, target_size_=target_size, path_=IMG_PATH, p=0.5, train_mode=False):\n",
    "        self.target_size = target_size_\n",
    "        self.path = path_\n",
    "        self.data_list = data_list_\n",
    "        self.p = p\n",
    "\n",
    "        self.mean = np.array([0.485, 0.456, 0.406]).reshape((3, 1, 1))\n",
    "        self.std = np.array([0.229, 0.224, 0.225]).reshape((3, 1, 1))\n",
    "\n",
    "        self.train_mode = train_mode\n",
    "        self.flip = RandomHorizontalFlip(p)\n",
    "        self.contrast = RandomContrast(p)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data_list[idx]\n",
    "\n",
    "        img_str = self.path + item.filename\n",
    "        img = display.read_img(img_str, self.target_size)\n",
    "        img = img / 255.0\n",
    "\n",
    "        gt = np.vstack(item.bounding_boxes)\n",
    "\n",
    "        if self.train_mode:\n",
    "            img = self.contrast(img)\n",
    "            img, gt = self.flip(img, gt)\n",
    "\n",
    "        img = img.transpose((2, 0, 1))\n",
    "        img = (img - self.mean) / self.std\n",
    "        img = torch.from_numpy(img).float()\n",
    "\n",
    "        gt = torch.from_numpy(gt).float()\n",
    "        gt = center_to_minmax(gt)        \n",
    "\n",
    "        c = np.array(item.class_id) + 1\n",
    "        c = torch.from_numpy(np.array(item.class_id)) + 1\n",
    "\n",
    "        return (img, gt, c)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Writing custom collector function since the Dataset class returns both tensors and lists.\n",
    "    \"\"\"\n",
    "\n",
    "    x = [b[0] for b in batch]\n",
    "    x = torch.stack(x, dim=0)\n",
    "    gt = [b[1] for b in batch]\n",
    "    c = [b[2] for b in batch]\n",
    "    return (x, gt, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_size = 0.9\n",
    "\n",
    "split_idx = int(train_size * len(data_list))\n",
    "\n",
    "train_dataset = PascalData(data_list[0:split_idx], train_mode=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, collate_fn=collate_fn)\n",
    "\n",
    "val_dataset = PascalData(data_list[split_idx:])\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProposalLayer(nn.Module):\n",
    "    def __init__(self, img_size, nms_thresh=0.7, n_train_pre_nms=2000, n_train_post_nms=1000,\n",
    "                 n_test_pre_nms=600, n_test_post_nms=300, min_size=16):\n",
    "\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.nms_threshold = nms_thresh\n",
    "        self.n_train_pre_nms = n_train_pre_nms\n",
    "        self.n_train_post_nms = n_train_post_nms\n",
    "        self.n_test_pre_nms = n_test_pre_nms\n",
    "        self.n_test_post_nms = n_test_post_nms\n",
    "        self.min_size = min_size\n",
    "\n",
    "    def forward(self, locs, scores, anchors):\n",
    "        if self.training:\n",
    "            n_pre_nms = self.n_train_pre_nms\n",
    "            n_post_nms = self.n_train_post_nms\n",
    "        else:\n",
    "            n_pre_nms = self.n_test_pre_nms\n",
    "            n_post_nms = self.n_test_post_nms\n",
    "\n",
    "        roi = loc2bbox(anchors, locs)\n",
    "        roi = torch.clamp(roi, 0, self.img_size)\n",
    "\n",
    "        # Remove predicted boxes with either height or width < threshold.\n",
    "        widths = roi[:, 2] - roi[:, 0]\n",
    "        heights = roi[:, 3] - roi[:, 1]\n",
    "        keep = torch.where((heights >= self.min_size) & (widths >= self.min_size))[0]\n",
    "\n",
    "        roi = roi[keep, :]\n",
    "        scores = scores[keep]\n",
    "\n",
    "        # Sort all (proposal, score) pairs by score from highest to lowest and the the n_pre_nms highest.        \n",
    "        indicies = torch.argsort(scores, descending=True)        \n",
    "        indicies = indicies[:n_pre_nms]\n",
    "        roi = roi[indicies, :]\n",
    "        scores = scores[indicies]\n",
    "\n",
    "        keep = nms(roi, scores, self.nms_threshold)\n",
    "        keep = keep[:n_post_nms]\n",
    "        roi = roi[keep]\n",
    "\n",
    "        return roi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegionProposalNetwork(nn.Module):\n",
    "    def __init__(self, img_size, in_channels=256, mid_channels=256, ratios=[0.7, 1, 1.3],\n",
    "                 anchor_scales=[4, 8, 16], sub_sample=16):\n",
    "        super().__init__()\n",
    "\n",
    "        # Note: all images in the forward pass need to be of shape 3 x img_size x img_size\n",
    "        self.img_size = img_size\n",
    "        self.anchors = generator_anchors(img_size, sub_sample, anchor_scales=anchor_scales, ratios=ratios)\n",
    "        self.num_base_anchors = len(ratios) * len(anchor_scales)\n",
    "\n",
    "        self.proposal_layer = ProposalLayer(img_size)\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels, mid_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.score = nn.Conv2d(mid_channels, 2 * self.num_base_anchors, kernel_size=1, stride=1, padding=0)\n",
    "        self.loc = nn.Conv2d(mid_channels, 4 * self.num_base_anchors, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        normal_init(self.conv, 0.0, 0.01)\n",
    "        normal_init(self.score, 0.0, 0.01)\n",
    "        normal_init(self.loc, 0.0, 0.01)\n",
    "\n",
    "    def forward(self, x):        \n",
    "        n, _, height, width = x.shape        \n",
    "        h = F.relu(self.conv(x))\n",
    "\n",
    "        rpn_locs = self.loc(h)\n",
    "        rpn_locs = rpn_locs.permute(0, 2, 3, 1).contiguous().view(n, -1, 4)\n",
    "\n",
    "        rpn_scores = self.score(h)\n",
    "        rpn_scores = rpn_scores.permute(0, 2, 3, 1).contiguous()\n",
    "\n",
    "        rpn_softmax_scores = F.softmax(rpn_scores.view(n, height, width, self.num_base_anchors, 2), dim=4)\n",
    "\n",
    "        rpn_fg_scores = rpn_softmax_scores[:, :, :, :, 1].contiguous()\n",
    "        rpn_fg_scores = rpn_fg_scores.view(n, -1)\n",
    "        rpn_scores = rpn_scores.view(n, -1, 2)\n",
    "\n",
    "        rois = []\n",
    "        roi_indices = []\n",
    "        for i in range(n):\n",
    "            roi = self.proposal_layer(rpn_locs[i], rpn_fg_scores[i], self.anchors)\n",
    "            batch_index = i * torch.ones((len(roi),)).int()\n",
    "            rois.append(roi)\n",
    "            roi_indices.append(batch_index)\n",
    "\n",
    "        rois = torch.cat(rois, dim=0).to(device)\n",
    "        roi_indices = torch.cat(roi_indices, dim=0).to(device)\n",
    "        return rpn_locs, rpn_scores, rois, roi_indices, self.anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoIHead(nn.Module):\n",
    "    def __init__(self, num_classes, output_size, num_hidden=256, in_channels=256, sub_sample=16):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear = nn.Linear(output_size * output_size * in_channels, num_hidden)\n",
    "        self.cls_loc = nn.Linear(num_hidden, 4 * num_classes)\n",
    "        self.score = nn.Linear(num_hidden, num_classes)\n",
    "        self.roi_pool = RoIPool(output_size, spatial_scale=1/sub_sample)\n",
    "\n",
    "        normal_init(self.cls_loc, 0, 0.001)\n",
    "        normal_init(self.score, 0, 0.01)\n",
    "\n",
    "    def forward(self, x, rois, roi_indices):\n",
    "        indicies_and_rois = torch.cat([\n",
    "            roi_indices.float().unsqueeze(-1),\n",
    "            rois\n",
    "        ], dim=1)\n",
    "\n",
    "        pool_vals = self.roi_pool(x, indicies_and_rois)\n",
    "        pool_vals = pool_vals.view(pool_vals.size(0), -1)\n",
    "\n",
    "        h = F.relu(self.linear(pool_vals))\n",
    "        roi_cls_locs = self.cls_loc(h) * 224\n",
    "        roi_scores = self.score(h)\n",
    "        return roi_cls_locs, roi_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FasterRCNN(nn.Module):\n",
    "    def __init__(self, img_size, num_classes, output_size=7):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.img_size = img_size\n",
    "        self.num_classes = num_classes\n",
    "        pretrained_model = list(models.resnet34(pretrained='imagenet').children())[:-3]\n",
    "        self.backbone = nn.Sequential(*pretrained_model)\n",
    "        self.region_proposal_network = RegionProposalNetwork(img_size, sub_sample=16)\n",
    "        self.roi_head = RoIHead(num_classes, output_size)\n",
    "\n",
    "    def forward_rpn(self, x):\n",
    "        feature_map = self.backbone(x)\n",
    "        rpn_locs, rpn_scores, rois, roi_indices, _ = self.region_proposal_network(feature_map)\n",
    "        return rpn_locs, rpn_scores, rois, roi_indices, feature_map\n",
    "\n",
    "    def forward_roi(self, x, rois, roi_indices):\n",
    "        roi_cls_locs, roi_scores = self.roi_head(x, rois, roi_indices)\n",
    "        return roi_cls_locs, roi_scores\n",
    "    \n",
    "    def predict(self, x):\n",
    "        # Forward for inference\n",
    "        # Currently only support batch_size=1\n",
    "        assert len(x.shape) == 4 and x.shape[0] == 1\n",
    "        \n",
    "        rpn_locs, rpn_scores, rois, roi_indices, feature_map = self.forward_rpn(x)\n",
    "        roi_cls_locs, roi_scores = self.forward_roi(feature_map, rois, roi_indices)\n",
    "        \n",
    "        n_sample = roi_cls_locs.shape[0]\n",
    "        roi_cls_locs = roi_cls_locs.view(-1, self.num_classes, 4)\n",
    "        rois = rois.view(-1, 1, 4).expand_as(roi_cls_locs)\n",
    "                \n",
    "        cls_boxes = loc2bbox(rois.reshape(-1, 4), roi_cls_locs.reshape(-1, 4))\n",
    "        cls_boxes = cls_boxes.view(-1, self.num_classes * 4)\n",
    "        cls_boxes[:, 0::2] = torch.clamp(cls_boxes[:, 0::2], min=0, max=self.img_size)\n",
    "        cls_boxes[:, 1::2] = torch.clamp(cls_boxes[:, 1::2], min=0, max=self.img_size)\n",
    "        cls_boxes =  cls_boxes.view(-1,  self.num_classes, 4)\n",
    "\n",
    "        return roi_scores, cls_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anchor_labels(anchors, gt, img_size, pos_iou_threshold=0.7, neg_iou_threshold=0.3):\n",
    "    index_inside = torch.where((anchors[:, 0] >= 0) &\n",
    "                               (anchors[:, 1] >= 0) &\n",
    "                               (anchors[:, 2] <= img_size) &\n",
    "                               (anchors[:, 3] <= img_size))[0]\n",
    "\n",
    "    labels = -1 * torch.ones((len(index_inside), )).int()\n",
    "    valid_anchor_boxes = anchors[index_inside]\n",
    "\n",
    "    # TODO: remove * img_size\n",
    "    ious = jaccard(gt, valid_anchor_boxes)\n",
    "\n",
    "    # Which gt has the highest iou, for each anchor\n",
    "    argmax_ious = ious.argmax(axis=0)    \n",
    "    max_ious = ious[argmax_ious, np.arange(ious.shape[1])]        \n",
    "\n",
    "    # Which anchor has the highest iou, for each gt\n",
    "    gt_argmax_ious = ious.argmax(axis=1)    \n",
    "    gt_max_ious = ious[np.arange(ious.shape[0]), gt_argmax_ious]\n",
    "\n",
    "    labels[max_ious < neg_iou_threshold] = 0\n",
    "    labels[max_ious >= pos_iou_threshold] = 1\n",
    "    labels[gt_argmax_ious] = 1\n",
    "\n",
    "    return labels, valid_anchor_boxes, argmax_ious, index_inside\n",
    "\n",
    "\n",
    "def sample_labels(labels, n_sample=256, pos_ratio=0.5):\n",
    "    n_pos = pos_ratio * n_sample\n",
    "\n",
    "    pos_index = torch.where(labels == 1)[0]\n",
    "    if len(pos_index) > n_pos:\n",
    "        disable_index = random_choice(pos_index, size=(len(pos_index) - n_pos))\n",
    "        labels[disable_index] = -1\n",
    "\n",
    "    n_neg = n_sample - torch.sum(labels == 1)\n",
    "    neg_index = torch.where(labels == 0)[0]\n",
    "    if len(neg_index) > n_neg:\n",
    "        disable_index = random_choice(neg_index, size=(len(neg_index) - n_neg))\n",
    "        labels[disable_index] = -1\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def create_target_labels(rois, gt_boxes, label):\n",
    "    n_sample = 128\n",
    "    pos_ratio = 0.25\n",
    "    pos_iou_thresh = 0.5\n",
    "    neg_iou_thresh_hi = 0.5\n",
    "    neg_iou_thresh_lo = 0.0\n",
    "    loc_normalize_mean = torch.tensor([0.0, 0.0, 0.0, 0.0]).view((1, 4)).float().to(device)\n",
    "    loc_normalize_std = torch.tensor([0.1, 0.1, 0.2, 0.2]).view((1, 4)).float().to(device)\n",
    "\n",
    "    # Rois comes from the network, we need to disable the grad tracing,\n",
    "    # since we do some ops which are not differentiable\n",
    "    with torch.no_grad():          \n",
    "        pos_roi_per_image = np.round(n_sample * pos_ratio)\n",
    "        iou = jaccard(rois, gt_boxes)\n",
    "\n",
    "        gt_assignment = iou.argmax(dim=1)\n",
    "        max_iou = iou.max(axis=1)[0]\n",
    "\n",
    "        # Assign index 0 to background\n",
    "        gt_roi_label = label[gt_assignment] + 1\n",
    "\n",
    "        pos_index = torch.where(max_iou >= pos_iou_thresh)[0]\n",
    "        pos_roi_per_this_image = int(min(pos_roi_per_image, len(pos_index)))\n",
    "\n",
    "        if len(pos_index) > 0:\n",
    "            pos_index = random_choice(pos_index, pos_roi_per_this_image)\n",
    "\n",
    "        neg_index = torch.where((max_iou < neg_iou_thresh_hi) & (max_iou >= neg_iou_thresh_lo))[0]\n",
    "        neg_roi_per_this_image = n_sample - pos_roi_per_this_image\n",
    "        neg_roi_per_this_image = int(min(neg_roi_per_this_image, len(neg_index)))\n",
    "\n",
    "        if len(neg_index) > 0:\n",
    "            neg_index = random_choice(neg_index, neg_roi_per_this_image)\n",
    "\n",
    "        keep_index = torch.cat([pos_index, neg_index], dim=0)\n",
    "\n",
    "        gt_roi_label = gt_roi_label[keep_index]\n",
    "        gt_roi_label[pos_roi_per_this_image:] = 0  # negative labels becomes background\n",
    "        sample_roi = rois[keep_index]\n",
    "\n",
    "        gt_roi_loc = bbox2loc(sample_roi, gt_boxes[gt_assignment[keep_index]])\n",
    "        gt_roi_loc = (gt_roi_loc - loc_normalize_mean) / loc_normalize_std\n",
    "    return sample_roi, gt_roi_loc, gt_roi_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FasterRCNN(target_size, num_classes + 1) # 1 class for background\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "lr = 1e-3\n",
    "wd = 0.0\n",
    "roi_lambda = 1.0\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_smooth_l1(target, pred, labels, ignore_idx=-1):\n",
    "    filter_target = target[labels != ignore_idx, :]\n",
    "    filter_pred = pred[labels != ignore_idx, :]\n",
    "    loss = F.smooth_l1_loss(filter_target, filter_pred, reduction=\"none\")\n",
    "    return torch.sum(loss) / len(filter_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = 0.0\n",
    "    for _, (x, gt, c) in enumerate(train_loader):        \n",
    "        model.zero_grad()\n",
    "\n",
    "        x = x.to(device)\n",
    "        rpn_locs, rpn_scores, rois, roi_indices, feature_map = model.forward_rpn(x)\n",
    "\n",
    "        rpn_cls_loss, rpn_loc_loss = 0.0, 0.0\n",
    "        roi_cls_loss, roi_loc_loss = 0.0, 0.0\n",
    "        for i in range(len(x)):\n",
    "            gt_i = gt[i].to(device)\n",
    "            c_i = c[i].to(device)\n",
    "            rois_i = rois[roi_indices == i]\n",
    "\n",
    "            # RPN loss\n",
    "            anchors = generator_anchors(target_size, sub_sample=16)\n",
    "            labels, valid_anchor_boxes, argmax_ious, index_inside = create_anchor_labels(anchors, gt_i, target_size)\n",
    "            labels = sample_labels(labels)\n",
    "            locs = bbox2loc(valid_anchor_boxes, gt_i[argmax_ious])\n",
    "\n",
    "            anchor_labels = -1 * torch.ones((len(anchors),)).int()\n",
    "            anchor_labels[index_inside] = labels\n",
    "            anchor_labels = anchor_labels.long().to(device)\n",
    "\n",
    "            anchor_locations = torch.zeros_like(anchors)\n",
    "            anchor_locations[index_inside, :] = locs\n",
    "            anchor_locations = anchor_locations.to(device)\n",
    "\n",
    "            rpn_cls_loss += F.cross_entropy(rpn_scores[i, :, :], anchor_labels, ignore_index = -1)\n",
    "            temp_loss = F.smooth_l1_loss(anchor_locations, rpn_locs[i, :, :], reduction=\"none\")\n",
    "            \n",
    "            label_filter = (anchor_labels > 0).float().unsqueeze(-1)\n",
    "            temp_loss = temp_loss * label_filter\n",
    "            rpn_loc_loss += torch.sum(temp_loss)/ torch.sum(label_filter)\n",
    "\n",
    "            # ROI loss\n",
    "            sample_roi, gt_roi_loc, gt_roi_label = create_target_labels(rois_i, gt_i, c_i)            \n",
    "\n",
    "            sample_roi_index = torch.zeros(len(sample_roi)).to(device)\n",
    "            roi_cls_loc, roi_score = model.forward_roi(\n",
    "                feature_map[i, :, :, :].unsqueeze(0),\n",
    "                sample_roi,\n",
    "                sample_roi_index)\n",
    "\n",
    "            roi_cls_loss += F.cross_entropy(roi_score, gt_roi_label)\n",
    "\n",
    "            n_sample = roi_cls_loc.shape[0]\n",
    "            roi_loc = roi_cls_loc.view(n_sample, -1, 4)\n",
    "            roi_loc = roi_loc[torch.arange(0, n_sample).long(), gt_roi_label]\n",
    "            roi_loc_loss += F.smooth_l1_loss(roi_loc, gt_roi_loc, reduction=\"sum\") / len(gt_roi_loc)\n",
    "\n",
    "        rpn_loss = rpn_cls_loss  + rpn_loc_loss\n",
    "        roi_loss = roi_cls_loss + roi_lambda * roi_loc_loss\n",
    "\n",
    "        batch_loss = (rpn_loss + roi_loss) / len(x)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += batch_loss.detach().cpu().numpy()    \n",
    "    train_loss = np.round(train_loss / len(train_loader), 6)\n",
    "    print(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard_threshold = 0.1\n",
    "\n",
    "for i in range(len(val_dataset)):\n",
    "    (x, bb_true, class_true) = val_dataset[i]\n",
    "    bb_true, x = bb_true.to(device), x.to(device)\n",
    "    class_true = class_true.squeeze(0) - 1 # -1 to convert it from 1-21 to 0-20\n",
    "\n",
    "    x = x[None, :, :, :]\n",
    "    class_hat, bb_hat = model(x)    \n",
    "    \n",
    "    prob, class_idx = torch.max(class_hat, dim=1)\n",
    "    bb_hat = bb_hat[torch.arange(0, len(class_idx)), class_idx, :]\n",
    "    \n",
    "    prob_val, prob_idx = torch.sort(prob, descending=True)\n",
    "    bb_hat = bb_hat[prob_idx, :]\n",
    "    class_idx = class_idx[prob_idx]\n",
    "\n",
    "    jacard_values = jaccard(bb_hat.squeeze(0), bb_true.squeeze(0))\n",
    "    \n",
    "    for j in range(len(class_true)):\n",
    "        overlap = (jacard_values[:, j] > jaccard_threshold).nonzero()\n",
    "        class_true_j = int(class_true[j].detach().cpu().numpy())\n",
    "\n",
    "        if len(overlap) > 0:\n",
    "            class_id = class_idx[overlap[:,0]]\n",
    "            prob = prob_val[overlap[:,0]]\n",
    "            \n",
    "            \n",
    "            \n",
    "            #prob, class_id = class_hat_j.max(1)\n",
    "            #prob, sort_index = torch.sort(prob, descending=True)\n",
    "            #class_id = class_id[sort_index].detach().cpu().numpy()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAP:\n",
    "    def __init__(self, model, dataset, jaccard_threshold, anchors):\n",
    "        self.jaccard_threshold = jaccard_threshold\n",
    "        self.model = model\n",
    "        self.eps = np.finfo(np.float32).eps\n",
    "        self.anchors = anchors\n",
    "        self.dataset = dataset\n",
    "\n",
    "    @staticmethod\n",
    "    def voc_ap(rec, prec):\n",
    "        \"\"\"Compute VOC AP given precision and recall with the VOC-07 11-point method.\"\"\"\n",
    "\n",
    "        ap = 0.0\n",
    "        for t in np.arange(0.0, 1.1, 0.1):\n",
    "            if np.sum(rec >= t) == 0:\n",
    "                p = 0.0\n",
    "            else:\n",
    "                p = np.max(prec[rec >= t])\n",
    "            ap = ap + p / 11.0\n",
    "        return ap\n",
    "\n",
    "    def __call__(self):\n",
    "        self.model.eval()\n",
    "        aps = defaultdict(list)\n",
    "\n",
    "        for i in range(len(self.dataset)):\n",
    "            (x, bb_true, class_true) = self.dataset[i]\n",
    "            class_true = class_true.squeeze(0) - 1 # -1 to convert it from 1-21 to 0-20\n",
    "\n",
    "            x = x[None, :, :, :]\n",
    "            class_hat, bb_hat = self.model(x)\n",
    "\n",
    "            #class_hat = class_hat[0, :, 1:].sigmoid()\n",
    "\n",
    "            bb_hat = invert_transformation(bb_hat.squeeze(0), self.anchors)\n",
    "            jacard_values = jaccard(bb_hat.squeeze(0), bb_true.squeeze(0))\n",
    "\n",
    "            for j in range(len(class_true)):\n",
    "                overlap = (jacard_values[:, j] > self.jaccard_threshold).nonzero()\n",
    "                class_true_j = int(class_true[j].detach().cpu().numpy())\n",
    "\n",
    "                if len(overlap) > 0:\n",
    "                    class_hat_j = class_hat[overlap[:,0], :]\n",
    "                    prob, class_id = class_hat_j.max(1)\n",
    "                    prob, sort_index = torch.sort(prob, descending=True)\n",
    "                    class_id = class_id[sort_index].detach().cpu().numpy()\n",
    "\n",
    "                    tp = np.zeros_like(class_id)\n",
    "                    fp = np.zeros_like(class_id)\n",
    "\n",
    "                    found = False\n",
    "                    for d in range(len(class_id)):\n",
    "                        if found or class_id[d] != class_true[j]:\n",
    "                            fp[d] = 1.0\n",
    "                        else:\n",
    "                            tp[d] = 1.0\n",
    "                            found = True\n",
    "\n",
    "                    fp = np.cumsum(fp)\n",
    "                    tp = np.cumsum(tp)\n",
    "\n",
    "                    rec = tp\n",
    "                    prec = tp / np.maximum(tp + fp, self.eps)\n",
    "\n",
    "                    temp_ap = MAP.voc_ap(rec, prec)\n",
    "                    aps[class_true_j].append(temp_ap)\n",
    "                else:\n",
    "                    aps[class_true_j].append(0)\n",
    "\n",
    "        res_list = []\n",
    "        for _, list_value in aps.items():\n",
    "            res_list.append(sum(list_value) / len(list_value))\n",
    "\n",
    "        return res_list, sum(res_list) / len(res_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp = MAP(model, val_dataset, 0.3, [])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
